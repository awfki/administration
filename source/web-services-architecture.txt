.. index:: http
.. index:: web services
.. index:: service architecture

========================
Web Service Architecture
========================

This document provides an overview of administration and architecture
of web-based services. Most "real" systems administration work
involves a fair amount of wrangling HTTP servers, so the core
information in this document will likely be useful to most. As a
secondary benefit, HTTP provides a good basis to address a number of
key service-administration topics and issues, which are relevant to
all systems administrators.

Technology Overview
-------------------

.. index:: http; protocol

HTTP Protocol
~~~~~~~~~~~~~

There's more to the Internet and to network services than HTTP and
"the web," and the moment for protesting the conflation of HTTP and
"the Internet," has passed. For whatever its worth, HTTP has become
the UNIX :term:`pipe` of the Internet. Web browsers, of course
implement HTTP, but for testing purposes you should familiarize
yourself with ``curl``, which is a versatile command line HTTP client
(among other protocols.)

There are a few terms with specific meanings in the context of
HTTP. Consider these basic concepts:

.. glossary::

   client
      The software that sends requests to a server and then receives
      data (a :term:`resource`) in response. HTTP clients (typically)
      place requests to servers listening on port 80. HTTPS clients
      pace requests to servers listening on port 443. There are many
      robust, native, HTTP client libraries for application
      developers.

   server
      A system that responds to client requests and returns
      resources. HTTP servers listen for requests on port 80; however,
      servers can run on any port.. HTTPS servers listen for
      requests on port 443 by default. Recently, a number of
      non-world-wide-web services have emerged that implement HTTP
      servers. (i.e. RESTful APIs, Application Servers, databases like
      CouchDB.)

   resource
      The content returned by the :term:`server` in response to a
      :term:`request`, except for that data which is in the
      :term:`header`.

   request
      The message sent from a :term:`client` to a :term:`server`
      HTTP defines several different types of requests, discussed in
      :ref:`HTTP requests <http-requests>`.

   status code
      All responses from an HTTP server, include a status code,
      described in :ref:`HTTP status codes <http-statuses>`.

   header
      The metadata transmitted with a given request or response. See
      :ref:`headers <http-headers>` for more information.

   httpd
      A daemon that implements the HTTP protocol. Historically,
      this has refered to the NCSA httpd and its successor the Apache
      HTTPD Server, though the name is generic, and can refer to any
      HTTP server, typically multi-purpose ones running on Unix-like
      systems.

.. _http-headers:

Headers
```````

HTTP transmits :term:`metadata` regarding content in :term:`key/value`
pairs called headers. Headers are largely arbitrary, though different
kinds of clients have different requirements around headers. Use
"``curl -I``" to return a list of headers. See the following headers
from ``tychoish.com``: ::

      Server: nginx/0.7.67
      Date: Sun, 06 Nov 2011 14:26:07 GMT
      Content-Type: text/html
      Content-Length: 8720
      Last-Modified: Fri, 04 Nov 2011 13:39:02 GMT
      Connection: keep-alive
      Accept-Ranges: bytes

HTTP also provides for a set of headers in the request. They take the
same key/value form as response headers, although the set of keys is
different.

.. _http-statuses:

Status (Error) Codes
````````````````````

When you run "``curl -I``"  the first response line provides the HTTP
status, the above example omitted the following status: ::

      HTTP/1.1 200 OK

This conveys the version of the HTTP protocol in use [#version]_ a
status code (e.g. 200,) and then some human intelligible translation
of this code. You already know code ``404`` that servers returns when
it can't find the resource requested. ``200`` is, as above, is he
status code for "resource returned successfully."

There are a few dozen HTTP codes, and while some administrators have
memorized the HTTP status codes, there is no need: In general, 300
series codes reflect a redirection (e.g. "the resource you're looking
for is somewhere else,") 400 series code reflect some sort of error or
problem that the server has with the request, and 500 series requests
reflect some sort of "internal error," usually related to the server's
configuration or state.

The following codes are useful to know on sight. Use a reference for
everything else:

========  ==============================================================
**Code**  **Meaning**
--------  --------------------------------------------------------------
  200     Everything's ok.
  301     Resource Moved Permanently. Update your bookmarks.
  302     Moved Temporarily. Don't update your bookmarks.
  400     Error in request syntax. Client's fault.
  401     Authorization required. Use HTTP Auth to return the resource
  403     Access Denied. Bad HTTP Auth credentials or permissions.
  404     Resource not found. Typo in the request, or data not found.
  410     Resource removed from server. This is infrequently used.
  418     I'm a tea pot. From an April Fools RFC, and socially useful.
  500     Internal server error. Check configuration and error logs.
  501     Requires unimplemented feature. Check config and error logs.
  502     Bad gateway. Check proxy configuration. Upstream server error.
  504     Gateway timeout. Proxy server not responding.
========  ==============================================================

Often server logs will return more useful information regarding the
state of the system.

..  [#version] All HTTP services are or attempts to comply with
    version 1.1.

.. _http-requests:

Requests
````````

HTTP provides a very full featured interface for interacting with
remote resources, although in common use, the ``GET``, ``POST``, and
``PUT`` request account for the overwhelming majority of
requests. Requests types are generally refered to as "methods."
htrict semantic adherence to HTTP methods is one of the defining
aspects of ":term:`REST`"

.. glossary::

   GET
      Fetch a resource from an HTTP server.

   PUT
      Upload a resource to an HTTP server. Often fails as a result of
      file permissions and server configurations, but don't assume
      that it *will* fail. Less common than :term:`POST`

   POST
      Send a response to a web pages. Submitting web-forms are
      conventionally implemented as POSTS.

   DELETE
      Remove a resource from an HTTP server. Often fails as a result of
      file permissions and server configurations, but don't assume
      that it *will* fail. Used infrequently prior to RESTful
      web APIs.

   HEAD
      Retrieve only the headers without fecthing the body of the
      request.

HTTP Abstractions
-----------------

Conceptually, it's best to think about HTTP in terms of static content
conveyed directly from the server's file system, thought the web
server, to the end-user's client. In truth, operations are more
complex, and most HTTP deployments make use of a clustered server and
server side processing.

There are a couple of very simple abstractions that most general
purpose web servers provide that make it easier to deploy web services
using HTTP: *Proxy handling* where a server will "pass" a request to
another server and *URL rewriting* where the server will map incoming
requests for :term:`resources <resource>` to different internal paths
and resources.

EDIT http abstractions

Load Balancing and Proxies
~~~~~~~~~~~~~~~~~~~~~~~~~~

Most general purpose web servers have the ability to :term:`proxy` or
forward incoming requests to different HTTP (or :term:`CGI`,
:term:`FastCGI` or similar) server. The proxying process requires only
minimal overhead on the part of the front end server and makes it
possible to host an entire domain or sub-domain using a cluster of
machines or have a single public IP address that can access the
resources of a group of machines. As a result proxies are essential
for scaling web services horizontally and most deployments of an
consequence will require the use of this abstraction. These simple
proxying configurations can be thought of as an instance of
:term:`partitioning` or :term:`horizontal scaling`.

Load-balancing, then, are proxy configurations where a single public
resource or set of resources is provided by more than one machine that
serve identical content. The proxy server in these situations must
distribute the requests among the nodes and (optionally) track
connections to ensure that nodes remain responsive and in some
configurations can ensure that connections from a single client are
consistently routed to the same back-end when possible. Load balancers
include the ability to distribute requests unevenly among the node if
systems have different capacities as well as different possible
responses to node failures. Load balanced architectures are simple
examples of :term:`replication` or :term:`vertical scaling`.

URL Rewriting
~~~~~~~~~~~~~

URL rewriting allows :program :`httpd` programs to accept requests for
resources that don't exist as named and process those requests in such
a way as to map the reformed requests to actual resources that the
server can fulfill. URL rewriting engines often support regular
expression matching, and can provide both transparent rewriting where
URLs are rewritten transparently for the client and do not require an
additional request, or as "redirect ions," which the client is aware
of and requires multiple requests. URL rewriting is helpful for making
URLs seem sensible for users (as in removing/changing file extensions
or reforming query strings,) for providing administrators the
flexibility to reorganize content and back end systems without
changing the presentation, and for moving resources without breaking
links. Most web development frameworks provide some level of URL
abstraction but having this ability in the web server can be very
powerful in many situations. While there are some quirks of every URL
rewriting system, they are roughly similar, and it's important to be
familiar with the rewriting system in the web server you use.

Web Server Fundamentals
-----------------------

EDIT introduction to web servers

HTTP and Static Content
~~~~~~~~~~~~~~~~~~~~~~~

HTTP is really designed to serve static content, and most
general-purpose web servers (and browsers) and optimized to do this
really efficiently. Web browsers are configured to make multiple
requests in parallel to download embeded content (i.e. images, style
sheets, JavaScript) or web pages "all at once," rather than
sequentially." General purpose HTTPDs are also pretty good at
efficiently serving this kind of content. The main things to remember
are:

- Make sure that you're not serving static content (i.e. anything that
  the web/application server needs to modify) from a
  low-volume/single-threaded application server. This is an easy one
  to miss depending on how your development/test environment is
  configured.

- Use some sort of caching service, if needed. It's an additional
  layer of complexity, but using a front-end caching proxy like
  `Varnish <https://www.varnish-cache.org/>`_ or `Squid
  <http://www.squid-cache.org/>`_ can cache data in RAM and return
  results more quickly, which is useful in certain kinds of
  high-volume situations with certain kinds of applications. Caches
  are great, but they don't solve underlying problems, and they add an
  additional layer of complexities.

- Make sure all resources/assets originate from the same domain, if
  possible. Use a "``static.example.net``" if necessary, but being
  consistent with your domain usage can help your browser cache things
  more effectively. It also makes it easier for *you* to understand
  your own setups later. Keep things simple and organized.

Serving static content with HTTP is straightforward, when you need to
dynamically assemble content per-request, a more complex system is
required. The kind of dynamic content you require and the kinds of
existing applications and tools that you want to use dictate your
architecture--to some extent--from here.

.. index:: web services; cgi
.. index:: cgi

.. _cgi-app-servers:

Common Gateway Interfaces
~~~~~~~~~~~~~~~~~~~~~~~~~

CGI, FastCGI, SCGI, PSGI, WSGI, and Rack are all protocols used by web
servers to communicate with applications. Simply, users place HTTP
requests with a web server (:term:`httpd`,) which creates or passes
the request to a separate process, which generates a response that it
hands back to the HTTP server that returns the result to the
user. While this seems a bit complex, in practice CGI and related
protocols have simple designs, robust tool-sets, are commonly
understood by many developers, and (generally) provide excellent
process/privilege segregation.

There are fundamental differences between these protocols, even though
their overall method of operation is similar. Consider the following
overview:

.. glossary::

   CGI
      Common Gateway Interface. CGI is the "original" script gateway
      protocol. CGI is simple and easy to implement, but every request
      requires the webserver to create a new process, or copy of the
      application in memory, for the length of the request. The
      per-request process creation and tear-down doesn't scale well
      with database connections and large request loads.

   FastCGI
      FastCGI attempts to solve the process creation/tear-down
      overhead, by daemonizing he application, resources can be reused
      (i.e. process initialization, database connections, etc.) which
      greatly increases performance over conventional
      :term:`CGI`. However, FastCGI is more complex to implement, and
      typically FastCGI application instances have lower
      request-per-second-per-instance capacities than HTTP servers,
      which creates a minor architectural challenge. Also, to deploy
      new application code, FastCGI processes need to be restarted
      which may interrupt client requests.

   WSGI
      Web Server Gateway Interface (sometimes pronounced *wisgy* or
      *wisky*.) WSGI provides a method for web applications to
      communicate with conventional HTTP servers. WSGI was developed
      by the Python community, and is typically used by applications
      written in this language, though the interface is not
      necessarily Python specific. WSGI is easy to use, though the
      exact method of deployment and operation varies slightly by
      implementation.

   PSGI
      Perl Web Server Gateway. PSGI provides an interface, *a la*
      :term:`WSGI` between Perl web applications and other CGI-like
      servers. Indeed, PSGI primarily describes a tool-set for writing
      web applications rather than a particular interface or protocol
      to web servers (as PSGI applications can be made to run with
      CGI, FastCGI or HTTP interfaces.)

   SCGI
      Simple Common Gateway Interface. SCGI is operationally similar
      to :term:`FastCGI`, but the protocol is designed to appear more
      like :term:`CGI` applications.

   Rack
      Rack is a Ruby-centric (and inspired :term:`PSGI`) web-server
      interface that provides an abstraction layer/interface between
      web servers and Ruby applications that "appears native" to Ruby
      developers.

While CGI and FastCGI defined dynamic applications from the earliest
days of HTTP and the web, the other above mentioned interface methods
seem largely emerged in the context of recent web application
development frameworks like "Ruby on Rails" and "Django."

.. _http-app-servers:

HTTP App Servers
~~~~~~~~~~~~~~~~

Recently, a class of application servers have emerged that implement
HTTP instead of some intermediate protocol. While very efficient for
serving dynamic content, they're less efficient for serving static
resources and cannot support heavy loads. As a result these
application servers are typically clustered behind a general purpose
``httpd`` that can proxy requests back to the application server. In
this respect, such servers are operationally similar to
:term:`FastCGI` application servers, but are easier to develop
applications for and are (theoretically) more simple
operationally. Examples of these kinds of application servers include:
Thin, Mongrel, Twisted, and Node.js.

Embeded Interpreters
~~~~~~~~~~~~~~~~~~~~

In contrast to the various web server/gateway interfaces, the other
major paradigm of web application deployment centers on emending the
program or the programming language itself within the webserver
process. Implementations vary by language and by webserver. Typically
these methods are *very* powerful, and *very* fast, but are
idiosyncratic. For a quite a while, these methods were the prevailing
practice for deploying dynamic content.

This practice is most common in context of the Apache HTTPD Server
with Perl (and ``mod_perl``) and PHP (``mod_php``). While there are
also Ruby (``mod_ruby``) and Python (``mod_python``) implementations
of these methods, development on these methods has been abandoned and
other methods are strongly preferred.

With the exception of ``mod_php``, the embeded interpreters all
require you to restart Apache when deploying new code. Additionally,
all code run by way of an interpreter embeded in the web server
process runs with the permissions of the web server. These operational
limitations make this approach less ideal for shared environments.

Because most of the "next wave," web application servers use some sort
of gateway interface or return HTTP itself, I fear the embeded option
is neglected unfairly. While there are limitations that you must
consider, there are a number of very good reasons to deploy
applications using Apache itself as the application server. Consider
the following:

- ``mod_perl`` is very efficient, and not only provides a way to run
  CGI-style scripts, but also exposes most of the operation of Apache
  to Perl-scripting. In some advanced cases this level of flexibility
  may provide enough benefit to indicate using ``mod_perl`` and
  Apache over other options.

- ``mod_php`` has comparable performance to other methods of running
  PHP scripts, and is significantly easier to deploy applications
  using ``mod_php`` than most other methods of deploying PHP. [#fpm]_
  Because ``mod_php`` is so easy to use, I suspect that most PHP code
  is developed in this environment: I suspect that a great deal of
  common conception that "PHP just works," is due to the ease of use
  of ``mod_php``

.. [#fpm] In the last couple of years, `PHP-FPM <http://php-fpm.org/>`_
   has made PHP much easier to run as :term:`FastCGI`.

.. index:: distributed systems; http
.. index:: scaling

Scaling HTTP Servers and Building Distributed Systems
-----------------------------------------------------

EDIT distributed server section

In most cases, web servers are pretty straightforward and have pretty
low resource requirements. However, in a number of situations web
servers face some scaling challenges: when faced with extraordinary
load, when they must generate dynamic content for each request can use
significantly greater resources often require additional resources,
when services are critical and all downtime must be prevented.

In nearly every case, contemporary web applications face greater and
more immediate problems with database scaling. See
":doc:`database-scaling`" for more information related to database
scaling and architecture.

If HTTP becomes a bottleneck, like databases, there's a progression of
measures that you can use to ensure that your system can deal with the
traffic that you expect to face. In general consider the following
process:

Begin by: moving the database engine to a separate system, and
ensuring that your method of serving dynamic content is finely
tuned. In many cases, the default configuration for most dynamic content
(CGI/Apache/etc.) is poorly tuned: the application server or the
``httpd`` has a low maximum connection threshold and connections are
refused before capacity is reached. Connection timeouts, application
timeouts, and approaches to concurrency (threading, forking,
event-driven, etc) can all impact performance and all need to be
understood and addressed before other approaches can be taken.

Real HTTP service begins with decoupling services along logical
boundaries, so that it's easy to increase application capacity and
capacity for static content separately. If your application or "site,"
depends on multiple applications and runtimes, make sure that the
services run distinctly, and that all components can run independently
and with minimal dependencies.

The key to making sure this all works in practice is to use some sort
of balancing-proxy-sever "*in front of*" the servers that provide your
core application and content. This layer makes it possible for users
of your service to have the experience of only using one system when
in fact the service is supplied by a cluster of systems. Ideally most
gross URL rewriting will be addressed at this level.

.. note::

   Because most application servers are single threaded, it makes
   sense to run some 2-4 application servers, per system (each on a
   distinct TCP port.)

   Typically, run one application server, or webserver worker process
   per core.

   Note that the number of "worker" processes for the :command:`nginx`
   server defaults to 1 for most distributions which means that it
   will only use one processor core unless the  configuration is
   modified.

It's possible to architect system with this eventuality in mind from
the very beginning using :term:`virtual hosting` and private networks,
and separating the application layer from the "front-end" HTTP
servers. Not that many people do this, and in many cases it's a lot of
overhead for flexibility that isn't practically useful, but "keep
things separated," is a good axiom for these kinds of system
architectures.

Once, the application and HTTP content is segregated, it's a
relatively simple matter to cluster specific components and add
capacity "horizontally." As the application layer becomes saturated,
deploy more instances of the server and use the proxy server to
distribute load among those nodes, and you can safely repeat this for
each component service.

.. note:: If your system supports or requires a higher standard of
   availability, it's also good to keep at least two front-end proxy
   servers in rotation at any given time, by adding multiple DNS
   records for a single hostname that point to multiple hosts running
   identical front-end services.

.. seealso:: While availability and scaling are not necessarily linked
   tasks, consider the material covered in ":doc:`high-availability`"
   when thinking about architecture.

.. index:: web servers

HTTPD Technologies
------------------

EDIT http technology introduction

Throughout the course of this document I've attempted to provide very
generic explanations of HTTP, web server implementations, and the
actual existing technology that you're likely to use. But the truth is
that ``httpd`` instances are not equivalent, not even roughly so.

To be fair, HTTP is pretty simple, and strictly speaking there's no
need for a big multi-purpose web server. It's totally possible to use
:term:`inetd`, and a little bit of code (probably you might as well do
this in C) to create your own ``httpd``. Everytime a request comes in,
``inetd`` spawns a copy of your daemon, the request is handled and the
process terminates. The HTTP protocol is pretty straightforward, so
the server is easy to implement and the binary is pretty small and you
can have total control over the behavior of the server by changing
some values in a header file (and recompiling, of course.) I'm aware
of at least one pretty high traffic site that does things in this
manner and it works. Surprisingly well. So that's an option, but
perhaps a bit beyond most of us mortals.

By way of conclusion, in this section I wish to provide an overview of
how you might go about choosing a web server (or servers!) for your
own project and providing an introduction to five servers with which I
think every systems administrator in the early 21st century should be
familiar.

Choosing a Web Server
~~~~~~~~~~~~~~~~~~~~~

You should evaluate web servers on a couple of dimensions, including
RAM usage, configuration method, compatibility and interoperability
with your application servers, and resource utilization under load. In
turn:

- RAM usage, covers the amount of memory that the server uses at idle
  or under light load. Typically servers with a lot of extra embeded
  functionality (modules, etc.) will use more RAM, and typically
  unless something's wrong, this value isn't terribly important, but
  all other things being equal a server that idles lower is probably
  more desirable.

- Configuration, describes how easy and :term:`grokable` the server's
  configuration system is for you. This is a personal decision, but
  there are some configuration systems that seem built around
  particular tasks. For instance, some kinds of pragmatically
  configured virtual hosting schemes are considerably easier to setup
  and maintain with Lighttpd than other servers. Apache has probably
  the most well documented configuration interface of any
  server. Consider your task and be familiar with how different
  servers approach configuration.

- Compatibility and Interoperability, addresses the ease with which
  the server can connect to or run your application or service. The
  advent of CGI, FastCGI and successor inter-service protocols make
  this nearly a non-issue. At the same time, there are some
  operational reasons to use certain servers over others. For
  instance: the ``uwsgi`` application server is considerably easier to
  run with nginx than with Apache, while PHP code has historically
  been easier to run under Apache than with any other server, and
  there are any number of Apache modules that make Apache a clear
  winner in many circumstances.

- Resource utilization under load is, in many circumstances, the most
  important factor that you should consider when choosing an HTTP
  daemon. Because CPU and RAM use is tied directly to cost and
  capacity, a web server that uses fewer resources is preferable. At
  the same time, given a low typical concurrency rate, most web
  servers do not consume a large amount of resources under
  load. Nevertheless, understand your concurrency requirements and the
  ways in which web servers address concurrency and how these
  approaches affect resource utilization.

Approaches to Concurrency in Web Servers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

There are three basic approaches to dealing with concurrent requests
used by web servers: forking, threading, and queuing. This means:

- **Forking**. The example using :term:`inetd` above, is a primal
  example of this kind of methodology. In essence the server creates a
  new copy of the process for every request. This is simple because
  there are no requirements for a shared state between any of the
  processes that fulfill each :term:`request`. This approach is
  robust, but is resource intensive as the entire process is
  duplicated in memory per request. Apache HTTPD in the 1.x series was
  a *forking* web server, and continues to provide an optional forking
  implementation in the 2.x series.

- **Threading**. Functionally similar to the forking approach
  described above; however, rather than creating a new process for
  every request the web server creates a new thread to track each
  request. This is somewhat less memory intensive than the forking
  approach, but is more complicated from an engineering prospective
  which lead to some stability issues in early stage implementations
  and continues to impact compatibility with some embeded
  interpreters. Furthermore the performance advantages obtained with
  the threaded approach provide only modest benefits over forking and
  have real limitations at scale. The default "worker" "mpm module"
  for the 2.x series of the Apache HTTPD uses a threaded approach.

- **Queuing**. This method places all requests in a queue, and a uses
  an asynchronous event loop to service all requests. This method uses
  very little memory, particularly under load. This method is used by
  the "mpm_event" module for Apache, and more notably for the
  Lighttpd, nginx, and Cherokee web servers. These servers are
  sometimes described as "event driven" or "asynchronous."

The HTTPD Milieu
~~~~~~~~~~~~~~~~

Apache HTTPD
````````````

This article has included a number of references to the Apache HTTP
Server, and it's difficult to talk about HTTP or even open source and
Linux without considering the impact of the Apache ``httpd``. Apache
is descended from the original ``httpd`` developed at the
:term:`NCSA`. The server is highly modular and incredibly flexible,
having grown out of a series of "patches" (hence the name from, "a
patchy web server") to the original HTTPD. The Apache project
consolidated in the 1990s, and is generally regarded as one of the
early technological successes of open source, and likely fueled most
early adoption of GNU/Linux systems.

Today, the server itself remains popular and very useful, with most
administrators having some level of familiarity with Apache and its
configuration. It is well documented supported on every platform and
with every tool as a result of its wide adoption, and incredibly
robust as a result of it's extensive use. At the same time, Apache is
not a particularly efficient server, particularly in light of recent
competitors. While this comparison is frequently offered in analysis,
it's probably the case that it's overblown. The demands on the vast
majority of web servers will never surpass the ability of a well tuned
Apache instance on even modest hardware.

nginx
`````

nginx (*pronounced "engine x"*) is my personal favorite web
server. It's simple, functionally complete, uses very little memory,
and preforms reliably under all circumstances that I've been able to
throw at it. The configuration syntax is simple and makes many of the
more complicated Apache configurations simple. Many appreciate its and
aptitude for serving as an HTTP proxy and software load-balancer, and
it can serve as a high-volume Mail proxy for high volume mail
servers. The best part of nginx is that it just works.

While there are edge cases where it makes sense to use another server
(typically Apache,) and there are some edges that are more rough than
I'd like (more efficient handling of CGI, [#cgi]_ better authentication
systems, [#digest]_ or a pluggable caching layer, I can really see no
reason to *not* use nginx for any deployment.

.. [#cgi] Admittedly, the problem is largely with CGI itself. Given an
   option, I tend to prefer nginx's externalization of this and it's
   configuration of FastCGI processes.

.. [#digest] nginx only supports basic HTTP authentication. This is a
   fundamental flaw with HTTP, but support for digest authentication
   would at the very least be nice.

Lighttpd
````````

Lighttpd (*pronounced "lighty"*) was one of the first to use the
queuing methodology, and because of a lot of early stability and early
notable deployments (including Reddit,) was a favorite. In addition to
generally efficient operation, it offers a minimalist Lua-based
configuration which permits dynamic virtual host configuration.
Lighttpd, like nginx, supports FastCGI naively, and developed the
widely used "spawn-fcgi" tool for starting FastCGI servers.

Unfortunately, development on Lighttpd has stalled and there is a
persistent memory-leak issue which forces administrators to restart
the server every couple of days. Since late 2009 I don't think that
there has been any reason to use Lighttpd, except if you need the easy
virtual host configuration, can't find similar functionality in other
tools *and* don't mind restarting the server arbitrarily for a known
problem that isn't (and likely won't) be fixed.

AntiWeb
```````

The inclusion of `AntiWeb <http://hoytech.com/antiweb/>`_ is something
of an outlier, but I think it's a cool project and it fits in well
here and may be a good introduction to HTTP servers for someone
interested in the technology at a lower level. AntiWeb is a Common
Lisp web server that uses the event-driven/asynchronous approach like
Lighttpd or nginx, but it inherits some pretty innovative ideas
regarding web development and HTTP from the Lisp world. While you
might not use AntiWeb as your next ``httpd``, it's worth investigation
by anyone whose interested in web servers and web applications.

Cherokee
````````

I nearly didn't include Cherokee in this listing. Cherokee views
itself as the successor to Apache (hence the name,) combining Apache's
ease of use with the performance of event-driven servers like
nginx. Everything that I've seen indicates that it's a great software
with great performance. It's main selling point, apparently, is easy
configuration, which it accomplishes by way of a web-based
interface. Worth considering.
